{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "d:\\Users\\Nicholas\\Projects\\NLP\\fake_news\n"
    }
   ],
   "source": [
    "from project_path import project_path\n",
    "project_path(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, auc, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from src\n",
    "import src.models.models as models\n",
    "import src.models.data_loader as data_loader\n",
    "import src.models.training as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<torch._C.Generator at 0x1f806f062d0>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# seeds\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\n"
    }
   ],
   "source": [
    "# if gpu is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directories\n",
    "proj_dir = Path.cwd().parents[0]\n",
    "data_raw = Path(proj_dir, 'data', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake = pd.read_csv(Path(data_raw, 'Fake.csv'))\n",
    "data_true = pd.read_csv(Path(data_raw, 'True.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake['target'] = 1\n",
    "data_true['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_fake, data_true])\n",
    "data_sample, _ = train_test_split(data, train_size=0.1, random_state=RANDOM_SEED, shuffle=True, stratify=data['target'])\n",
    "data_sample = data_sample.rename(columns = {'text':'content'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    data_sample, \n",
    "    train_size=0.8, \n",
    "    random_state=RANDOM_SEED, \n",
    "    shuffle=True,\n",
    "    stratify=data_sample['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val, df_test = train_test_split(\n",
    "    df_test, \n",
    "    train_size=0.5, \n",
    "    random_state=RANDOM_SEED, \n",
    "    shuffle=True, \n",
    "    stratify=df_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3591, 5) (449, 5) (449, 5)\n"
    }
   ],
   "source": [
    "print(df_train.shape, df_val.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1    0.522974\n0    0.477026\nName: target, dtype: float64\n"
    }
   ],
   "source": [
    "print(df_train.target.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = \"D:/Users/Nicholas/Projects/BERT_pretrained/biobert-base-cased-v1.1\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512 * 3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = data_loader.create_data_loader(\n",
    "    df = df_train, \n",
    "    tokenizer = tokenizer, \n",
    "    max_len = max_len, \n",
    "    batch_size = batch_size, \n",
    "    chunksize = 512, \n",
    "    sampler = None, \n",
    "    shuffle = True, \n",
    "    drop_last = True)\n",
    "\n",
    "val_data_loader = data_loader.create_data_loader(\n",
    "    df = df_val, \n",
    "    tokenizer = tokenizer, \n",
    "    max_len = max_len, \n",
    "    batch_size = batch_size, \n",
    "    chunksize = 512, \n",
    "    sampler = None, \n",
    "    shuffle = False, \n",
    "    drop_last = False)\n",
    "\n",
    "test_data_loader = data_loader.create_data_loader(\n",
    "    df = df_test, \n",
    "    tokenizer = tokenizer, \n",
    "    max_len = max_len, \n",
    "    batch_size = batch_size, \n",
    "    chunksize = 512, \n",
    "    sampler = None, \n",
    "    shuffle = False, \n",
    "    drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    'PRE_TRAINED_MODEL_NAME':PRE_TRAINED_MODEL_NAME, \n",
    "    'n_classes':2, \n",
    "    'add_linear':[512,256], \n",
    "    'attn_bias':False, \n",
    "    'freeze_layer_count':8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at D:/Users/Nicholas/Projects/BERT_pretrained/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
    }
   ],
   "source": [
    "model = models.HIBERT(**params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bert.embeddings.word_embeddings.weight False\nbert.embeddings.position_embeddings.weight False\nbert.embeddings.token_type_embeddings.weight False\nbert.embeddings.LayerNorm.weight False\nbert.embeddings.LayerNorm.bias False\nbert.encoder.layer.0.attention.self.query.weight False\nbert.encoder.layer.0.attention.self.query.bias False\nbert.encoder.layer.0.attention.self.key.weight False\nbert.encoder.layer.0.attention.self.key.bias False\nbert.encoder.layer.0.attention.self.value.weight False\nbert.encoder.layer.0.attention.self.value.bias False\nbert.encoder.layer.0.attention.output.dense.weight False\nbert.encoder.layer.0.attention.output.dense.bias False\nbert.encoder.layer.0.attention.output.LayerNorm.weight False\nbert.encoder.layer.0.attention.output.LayerNorm.bias False\nbert.encoder.layer.0.intermediate.dense.weight False\nbert.encoder.layer.0.intermediate.dense.bias False\nbert.encoder.layer.0.output.dense.weight False\nbert.encoder.layer.0.output.dense.bias False\nbert.encoder.layer.0.output.LayerNorm.weight False\nbert.encoder.layer.0.output.LayerNorm.bias False\nbert.encoder.layer.1.attention.self.query.weight False\nbert.encoder.layer.1.attention.self.query.bias False\nbert.encoder.layer.1.attention.self.key.weight False\nbert.encoder.layer.1.attention.self.key.bias False\nbert.encoder.layer.1.attention.self.value.weight False\nbert.encoder.layer.1.attention.self.value.bias False\nbert.encoder.layer.1.attention.output.dense.weight False\nbert.encoder.layer.1.attention.output.dense.bias False\nbert.encoder.layer.1.attention.output.LayerNorm.weight False\nbert.encoder.layer.1.attention.output.LayerNorm.bias False\nbert.encoder.layer.1.intermediate.dense.weight False\nbert.encoder.layer.1.intermediate.dense.bias False\nbert.encoder.layer.1.output.dense.weight False\nbert.encoder.layer.1.output.dense.bias False\nbert.encoder.layer.1.output.LayerNorm.weight False\nbert.encoder.layer.1.output.LayerNorm.bias False\nbert.encoder.layer.2.attention.self.query.weight False\nbert.encoder.layer.2.attention.self.query.bias False\nbert.encoder.layer.2.attention.self.key.weight False\nbert.encoder.layer.2.attention.self.key.bias False\nbert.encoder.layer.2.attention.self.value.weight False\nbert.encoder.layer.2.attention.self.value.bias False\nbert.encoder.layer.2.attention.output.dense.weight False\nbert.encoder.layer.2.attention.output.dense.bias False\nbert.encoder.layer.2.attention.output.LayerNorm.weight False\nbert.encoder.layer.2.attention.output.LayerNorm.bias False\nbert.encoder.layer.2.intermediate.dense.weight False\nbert.encoder.layer.2.intermediate.dense.bias False\nbert.encoder.layer.2.output.dense.weight False\nbert.encoder.layer.2.output.dense.bias False\nbert.encoder.layer.2.output.LayerNorm.weight False\nbert.encoder.layer.2.output.LayerNorm.bias False\nbert.encoder.layer.3.attention.self.query.weight False\nbert.encoder.layer.3.attention.self.query.bias False\nbert.encoder.layer.3.attention.self.key.weight False\nbert.encoder.layer.3.attention.self.key.bias False\nbert.encoder.layer.3.attention.self.value.weight False\nbert.encoder.layer.3.attention.self.value.bias False\nbert.encoder.layer.3.attention.output.dense.weight False\nbert.encoder.layer.3.attention.output.dense.bias False\nbert.encoder.layer.3.attention.output.LayerNorm.weight False\nbert.encoder.layer.3.attention.output.LayerNorm.bias False\nbert.encoder.layer.3.intermediate.dense.weight False\nbert.encoder.layer.3.intermediate.dense.bias False\nbert.encoder.layer.3.output.dense.weight False\nbert.encoder.layer.3.output.dense.bias False\nbert.encoder.layer.3.output.LayerNorm.weight False\nbert.encoder.layer.3.output.LayerNorm.bias False\nbert.encoder.layer.4.attention.self.query.weight False\nbert.encoder.layer.4.attention.self.query.bias False\nbert.encoder.layer.4.attention.self.key.weight False\nbert.encoder.layer.4.attention.self.key.bias False\nbert.encoder.layer.4.attention.self.value.weight False\nbert.encoder.layer.4.attention.self.value.bias False\nbert.encoder.layer.4.attention.output.dense.weight False\nbert.encoder.layer.4.attention.output.dense.bias False\nbert.encoder.layer.4.attention.output.LayerNorm.weight False\nbert.encoder.layer.4.attention.output.LayerNorm.bias False\nbert.encoder.layer.4.intermediate.dense.weight False\nbert.encoder.layer.4.intermediate.dense.bias False\nbert.encoder.layer.4.output.dense.weight False\nbert.encoder.layer.4.output.dense.bias False\nbert.encoder.layer.4.output.LayerNorm.weight False\nbert.encoder.layer.4.output.LayerNorm.bias False\nbert.encoder.layer.5.attention.self.query.weight False\nbert.encoder.layer.5.attention.self.query.bias False\nbert.encoder.layer.5.attention.self.key.weight False\nbert.encoder.layer.5.attention.self.key.bias False\nbert.encoder.layer.5.attention.self.value.weight False\nbert.encoder.layer.5.attention.self.value.bias False\nbert.encoder.layer.5.attention.output.dense.weight False\nbert.encoder.layer.5.attention.output.dense.bias False\nbert.encoder.layer.5.attention.output.LayerNorm.weight False\nbert.encoder.layer.5.attention.output.LayerNorm.bias False\nbert.encoder.layer.5.intermediate.dense.weight False\nbert.encoder.layer.5.intermediate.dense.bias False\nbert.encoder.layer.5.output.dense.weight False\nbert.encoder.layer.5.output.dense.bias False\nbert.encoder.layer.5.output.LayerNorm.weight False\nbert.encoder.layer.5.output.LayerNorm.bias False\nbert.encoder.layer.6.attention.self.query.weight False\nbert.encoder.layer.6.attention.self.query.bias False\nbert.encoder.layer.6.attention.self.key.weight False\nbert.encoder.layer.6.attention.self.key.bias False\nbert.encoder.layer.6.attention.self.value.weight False\nbert.encoder.layer.6.attention.self.value.bias False\nbert.encoder.layer.6.attention.output.dense.weight False\nbert.encoder.layer.6.attention.output.dense.bias False\nbert.encoder.layer.6.attention.output.LayerNorm.weight False\nbert.encoder.layer.6.attention.output.LayerNorm.bias False\nbert.encoder.layer.6.intermediate.dense.weight False\nbert.encoder.layer.6.intermediate.dense.bias False\nbert.encoder.layer.6.output.dense.weight False\nbert.encoder.layer.6.output.dense.bias False\nbert.encoder.layer.6.output.LayerNorm.weight False\nbert.encoder.layer.6.output.LayerNorm.bias False\nbert.encoder.layer.7.attention.self.query.weight False\nbert.encoder.layer.7.attention.self.query.bias False\nbert.encoder.layer.7.attention.self.key.weight False\nbert.encoder.layer.7.attention.self.key.bias False\nbert.encoder.layer.7.attention.self.value.weight False\nbert.encoder.layer.7.attention.self.value.bias False\nbert.encoder.layer.7.attention.output.dense.weight False\nbert.encoder.layer.7.attention.output.dense.bias False\nbert.encoder.layer.7.attention.output.LayerNorm.weight False\nbert.encoder.layer.7.attention.output.LayerNorm.bias False\nbert.encoder.layer.7.intermediate.dense.weight False\nbert.encoder.layer.7.intermediate.dense.bias False\nbert.encoder.layer.7.output.dense.weight False\nbert.encoder.layer.7.output.dense.bias False\nbert.encoder.layer.7.output.LayerNorm.weight False\nbert.encoder.layer.7.output.LayerNorm.bias False\nbert.encoder.layer.8.attention.self.query.weight True\nbert.encoder.layer.8.attention.self.query.bias True\nbert.encoder.layer.8.attention.self.key.weight True\nbert.encoder.layer.8.attention.self.key.bias True\nbert.encoder.layer.8.attention.self.value.weight True\nbert.encoder.layer.8.attention.self.value.bias True\nbert.encoder.layer.8.attention.output.dense.weight True\nbert.encoder.layer.8.attention.output.dense.bias True\nbert.encoder.layer.8.attention.output.LayerNorm.weight True\nbert.encoder.layer.8.attention.output.LayerNorm.bias True\nbert.encoder.layer.8.intermediate.dense.weight True\nbert.encoder.layer.8.intermediate.dense.bias True\nbert.encoder.layer.8.output.dense.weight True\nbert.encoder.layer.8.output.dense.bias True\nbert.encoder.layer.8.output.LayerNorm.weight True\nbert.encoder.layer.8.output.LayerNorm.bias True\nbert.encoder.layer.9.attention.self.query.weight True\nbert.encoder.layer.9.attention.self.query.bias True\nbert.encoder.layer.9.attention.self.key.weight True\nbert.encoder.layer.9.attention.self.key.bias True\nbert.encoder.layer.9.attention.self.value.weight True\nbert.encoder.layer.9.attention.self.value.bias True\nbert.encoder.layer.9.attention.output.dense.weight True\nbert.encoder.layer.9.attention.output.dense.bias True\nbert.encoder.layer.9.attention.output.LayerNorm.weight True\nbert.encoder.layer.9.attention.output.LayerNorm.bias True\nbert.encoder.layer.9.intermediate.dense.weight True\nbert.encoder.layer.9.intermediate.dense.bias True\nbert.encoder.layer.9.output.dense.weight True\nbert.encoder.layer.9.output.dense.bias True\nbert.encoder.layer.9.output.LayerNorm.weight True\nbert.encoder.layer.9.output.LayerNorm.bias True\nbert.encoder.layer.10.attention.self.query.weight True\nbert.encoder.layer.10.attention.self.query.bias True\nbert.encoder.layer.10.attention.self.key.weight True\nbert.encoder.layer.10.attention.self.key.bias True\nbert.encoder.layer.10.attention.self.value.weight True\nbert.encoder.layer.10.attention.self.value.bias True\nbert.encoder.layer.10.attention.output.dense.weight True\nbert.encoder.layer.10.attention.output.dense.bias True\nbert.encoder.layer.10.attention.output.LayerNorm.weight True\nbert.encoder.layer.10.attention.output.LayerNorm.bias True\nbert.encoder.layer.10.intermediate.dense.weight True\nbert.encoder.layer.10.intermediate.dense.bias True\nbert.encoder.layer.10.output.dense.weight True\nbert.encoder.layer.10.output.dense.bias True\nbert.encoder.layer.10.output.LayerNorm.weight True\nbert.encoder.layer.10.output.LayerNorm.bias True\nbert.encoder.layer.11.attention.self.query.weight True\nbert.encoder.layer.11.attention.self.query.bias True\nbert.encoder.layer.11.attention.self.key.weight True\nbert.encoder.layer.11.attention.self.key.bias True\nbert.encoder.layer.11.attention.self.value.weight True\nbert.encoder.layer.11.attention.self.value.bias True\nbert.encoder.layer.11.attention.output.dense.weight True\nbert.encoder.layer.11.attention.output.dense.bias True\nbert.encoder.layer.11.attention.output.LayerNorm.weight True\nbert.encoder.layer.11.attention.output.LayerNorm.bias True\nbert.encoder.layer.11.intermediate.dense.weight True\nbert.encoder.layer.11.intermediate.dense.bias True\nbert.encoder.layer.11.output.dense.weight True\nbert.encoder.layer.11.output.dense.bias True\nbert.encoder.layer.11.output.LayerNorm.weight True\nbert.encoder.layer.11.output.LayerNorm.bias True\nattention.attn.weight True\nattention.attn_combine.weight True\nfc.0.fc.weight True\nfc.0.fc.bias True\nfc.1.fc.weight True\nfc.1.fc.bias True\nfc.2.weight True\nfc.2.bias True\n"
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_data_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1 / 2\n--------------------\nBatch Train loss: 0.6941     Class 1 prop: 0.4688     ACC: 0.5938     F1: 0.6667     AUC: 0.5139     \n[1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0] [0.5120627  0.50814855 0.5102492  0.50960124 0.50968945 0.5126172\n 0.5120235  0.5110454  0.51118624 0.50971764 0.5135421  0.51240885\n 0.5129199  0.50987375 0.51148874 0.5100291  0.50921625 0.50675637\n 0.5121607  0.5127731  0.5106375  0.50939375 0.5121539  0.5080478\n 0.506956   0.51340556 0.51161134 0.5080251  0.5131007  0.5076514\n 0.512083   0.5103428 ]\n\nEpoch Train loss: 0.6941     ACC: 0.5938     F1: 0.6667     AUC: 0.5139     \nEpoch val  loss: 0.6946     ACC: 0.5000     F1: 0.0000     AUC: 0.2917     \n\nEpoch 2 / 2\n--------------------\nBatch Train loss: 0.6919     Class 1 prop: 0.4688     ACC: 0.7812     F1: 0.7879     AUC: 0.7479     \n[1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1] [0.50779    0.506229   0.5036712  0.5062225  0.50620043 0.50652605\n 0.5022381  0.50458634 0.50232977 0.5105971  0.50903    0.5039221\n 0.50875306 0.5088336  0.5086022  0.5074931  0.5042587  0.5081795\n 0.50471526 0.5036841  0.50471663 0.5042617  0.5048826  0.50956666\n 0.50545204 0.5074557  0.5050172  0.506135   0.5067745  0.50754243\n 0.50173277 0.5084404 ]\n\nEpoch Train loss: 0.6919     ACC: 0.7812     F1: 0.7879     AUC: 0.7479     \nEpoch val  loss: 0.6937     ACC: 0.5000     F1: 0.0000     AUC: 0.4167     \n\nWall time: 1min 58s\n"
    }
   ],
   "source": [
    "%%time\n",
    "history = training.train_model(epochs, model, train_data_loader, val_data_loader, loss_fn, optimizer, device, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(history, open('training_history.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "load() takes exactly 1 positional argument (2 given)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-856c60b3029a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training_history.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: load() takes exactly 1 positional argument (2 given)"
     ]
    }
   ],
   "source": [
    "history = pickle.load(history, open('training_history.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-701806d04afc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'val loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "plt.plot(list(range(epochs)), history['train_loss'], label = 'train loss')\n",
    "plt.plot(list(range(epochs)), history['val_loss'], label = 'val loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-bb13a9502384>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_f1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train f1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_f1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'val f1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "plt.plot(list(range(epochs)), history['train_f1'], label = 'train f1')\n",
    "plt.plot(list(range(epochs)), history['val_f1'], label = 'val f1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-8fd44e8f8df7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_auc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train auc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_auc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'val auc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "plt.plot(list(range(epochs)), history['train_auc'], label = 'train auc')\n",
    "plt.plot(list(range(epochs)), history['val_auc'], label = 'val auc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at D:/Users/Nicholas/Projects/BERT_pretrained/biobert-base-cased-v1.1 were not used when initializing BertModel: ['bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_model_state.bin'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-cd37501fce9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_trained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIBERT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_trained\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model_state.bin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbest_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_threshold.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_model_state.bin'"
     ]
    }
   ],
   "source": [
    "model_trained = models.HIBERT(**params_dict)\n",
    "model_trained.load_state_dict(torch.load('best_model_state.bin'))\n",
    "\n",
    "best_threshold = pickle.load(history, open('best_threshold.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'best_threshold' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-79bb0304d223>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdata_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     best_threshold = best_threshold)\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_threshold' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = training.pred_model(\n",
    "    model = model_trained, \n",
    "    data_loader = test_data_loader, \n",
    "    device = device, \n",
    "    best_threshold = best_threshold)"
   ]
  }
 ]
}