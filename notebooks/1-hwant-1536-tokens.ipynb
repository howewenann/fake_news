{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "d:\\Users\\Nicholas\\Projects\\NLP\\fake_news\n"
    }
   ],
   "source": [
    "from project_path import project_path\n",
    "project_path(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, auc, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from src\n",
    "import src.models.models as models\n",
    "import src.models.data_loader as data_loader\n",
    "import src.models.training as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<torch._C.Generator at 0x1c7e77c62d0>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# seeds\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\n"
    }
   ],
   "source": [
    "# if gpu is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directories\n",
    "proj_dir = Path.cwd().parents[0]\n",
    "data_raw = Path(proj_dir, 'data', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake = pd.read_csv(Path(data_raw, 'Fake.csv'))\n",
    "data_true = pd.read_csv(Path(data_raw, 'True.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake['target'] = 1\n",
    "data_true['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_fake, data_true])\n",
    "data_sample, _ = train_test_split(data, train_size=0.1, random_state=RANDOM_SEED, shuffle=True, stratify=data['target'])\n",
    "data_sample = data_sample.rename(columns = {'text':'content'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    data_sample, \n",
    "    train_size=0.8, \n",
    "    random_state=RANDOM_SEED, \n",
    "    shuffle=True,\n",
    "    stratify=data_sample['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val, df_test = train_test_split(\n",
    "    df_test, \n",
    "    train_size=0.5, \n",
    "    random_state=RANDOM_SEED, \n",
    "    shuffle=True, \n",
    "    stratify=df_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3591, 5) (449, 5) (449, 5)\n"
    }
   ],
   "source": [
    "print(df_train.shape, df_val.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1    0.522974\n0    0.477026\nName: target, dtype: float64\n"
    }
   ],
   "source": [
    "print(df_train.target.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = \"D:/Users/Nicholas/Projects/BERT_pretrained/biobert-base-cased-v1.1\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512 * 3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = data_loader.create_data_loader(\n",
    "    df = df_train, \n",
    "    tokenizer = tokenizer, \n",
    "    max_len = max_len, \n",
    "    batch_size = batch_size, \n",
    "    chunksize = 512, \n",
    "    sampler = None, \n",
    "    shuffle = True, \n",
    "    drop_last = True)\n",
    "\n",
    "val_data_loader = data_loader.create_data_loader(\n",
    "    df = df_val, \n",
    "    tokenizer = tokenizer, \n",
    "    max_len = max_len, \n",
    "    batch_size = batch_size, \n",
    "    chunksize = 512, \n",
    "    sampler = None, \n",
    "    shuffle = False, \n",
    "    drop_last = False)\n",
    "\n",
    "test_data_loader = data_loader.create_data_loader(\n",
    "    df = df_test, \n",
    "    tokenizer = tokenizer, \n",
    "    max_len = max_len, \n",
    "    batch_size = batch_size, \n",
    "    chunksize = 512, \n",
    "    sampler = None, \n",
    "    shuffle = False, \n",
    "    drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    'PRE_TRAINED_MODEL_NAME':PRE_TRAINED_MODEL_NAME, \n",
    "    'n_classes':2, \n",
    "    'add_linear':[512,256], \n",
    "    'attn_bias':False, \n",
    "    'freeze_layer_count':8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at D:/Users/Nicholas/Projects/BERT_pretrained/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
    }
   ],
   "source": [
    "model = models.HIBERT(**params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bert.embeddings.word_embeddings.weight False\nbert.embeddings.position_embeddings.weight False\nbert.embeddings.token_type_embeddings.weight False\nbert.embeddings.LayerNorm.weight False\nbert.embeddings.LayerNorm.bias False\nbert.encoder.layer.0.attention.self.query.weight False\nbert.encoder.layer.0.attention.self.query.bias False\nbert.encoder.layer.0.attention.self.key.weight False\nbert.encoder.layer.0.attention.self.key.bias False\nbert.encoder.layer.0.attention.self.value.weight False\nbert.encoder.layer.0.attention.self.value.bias False\nbert.encoder.layer.0.attention.output.dense.weight False\nbert.encoder.layer.0.attention.output.dense.bias False\nbert.encoder.layer.0.attention.output.LayerNorm.weight False\nbert.encoder.layer.0.attention.output.LayerNorm.bias False\nbert.encoder.layer.0.intermediate.dense.weight False\nbert.encoder.layer.0.intermediate.dense.bias False\nbert.encoder.layer.0.output.dense.weight False\nbert.encoder.layer.0.output.dense.bias False\nbert.encoder.layer.0.output.LayerNorm.weight False\nbert.encoder.layer.0.output.LayerNorm.bias False\nbert.encoder.layer.1.attention.self.query.weight False\nbert.encoder.layer.1.attention.self.query.bias False\nbert.encoder.layer.1.attention.self.key.weight False\nbert.encoder.layer.1.attention.self.key.bias False\nbert.encoder.layer.1.attention.self.value.weight False\nbert.encoder.layer.1.attention.self.value.bias False\nbert.encoder.layer.1.attention.output.dense.weight False\nbert.encoder.layer.1.attention.output.dense.bias False\nbert.encoder.layer.1.attention.output.LayerNorm.weight False\nbert.encoder.layer.1.attention.output.LayerNorm.bias False\nbert.encoder.layer.1.intermediate.dense.weight False\nbert.encoder.layer.1.intermediate.dense.bias False\nbert.encoder.layer.1.output.dense.weight False\nbert.encoder.layer.1.output.dense.bias False\nbert.encoder.layer.1.output.LayerNorm.weight False\nbert.encoder.layer.1.output.LayerNorm.bias False\nbert.encoder.layer.2.attention.self.query.weight False\nbert.encoder.layer.2.attention.self.query.bias False\nbert.encoder.layer.2.attention.self.key.weight False\nbert.encoder.layer.2.attention.self.key.bias False\nbert.encoder.layer.2.attention.self.value.weight False\nbert.encoder.layer.2.attention.self.value.bias False\nbert.encoder.layer.2.attention.output.dense.weight False\nbert.encoder.layer.2.attention.output.dense.bias False\nbert.encoder.layer.2.attention.output.LayerNorm.weight False\nbert.encoder.layer.2.attention.output.LayerNorm.bias False\nbert.encoder.layer.2.intermediate.dense.weight False\nbert.encoder.layer.2.intermediate.dense.bias False\nbert.encoder.layer.2.output.dense.weight False\nbert.encoder.layer.2.output.dense.bias False\nbert.encoder.layer.2.output.LayerNorm.weight False\nbert.encoder.layer.2.output.LayerNorm.bias False\nbert.encoder.layer.3.attention.self.query.weight False\nbert.encoder.layer.3.attention.self.query.bias False\nbert.encoder.layer.3.attention.self.key.weight False\nbert.encoder.layer.3.attention.self.key.bias False\nbert.encoder.layer.3.attention.self.value.weight False\nbert.encoder.layer.3.attention.self.value.bias False\nbert.encoder.layer.3.attention.output.dense.weight False\nbert.encoder.layer.3.attention.output.dense.bias False\nbert.encoder.layer.3.attention.output.LayerNorm.weight False\nbert.encoder.layer.3.attention.output.LayerNorm.bias False\nbert.encoder.layer.3.intermediate.dense.weight False\nbert.encoder.layer.3.intermediate.dense.bias False\nbert.encoder.layer.3.output.dense.weight False\nbert.encoder.layer.3.output.dense.bias False\nbert.encoder.layer.3.output.LayerNorm.weight False\nbert.encoder.layer.3.output.LayerNorm.bias False\nbert.encoder.layer.4.attention.self.query.weight False\nbert.encoder.layer.4.attention.self.query.bias False\nbert.encoder.layer.4.attention.self.key.weight False\nbert.encoder.layer.4.attention.self.key.bias False\nbert.encoder.layer.4.attention.self.value.weight False\nbert.encoder.layer.4.attention.self.value.bias False\nbert.encoder.layer.4.attention.output.dense.weight False\nbert.encoder.layer.4.attention.output.dense.bias False\nbert.encoder.layer.4.attention.output.LayerNorm.weight False\nbert.encoder.layer.4.attention.output.LayerNorm.bias False\nbert.encoder.layer.4.intermediate.dense.weight False\nbert.encoder.layer.4.intermediate.dense.bias False\nbert.encoder.layer.4.output.dense.weight False\nbert.encoder.layer.4.output.dense.bias False\nbert.encoder.layer.4.output.LayerNorm.weight False\nbert.encoder.layer.4.output.LayerNorm.bias False\nbert.encoder.layer.5.attention.self.query.weight False\nbert.encoder.layer.5.attention.self.query.bias False\nbert.encoder.layer.5.attention.self.key.weight False\nbert.encoder.layer.5.attention.self.key.bias False\nbert.encoder.layer.5.attention.self.value.weight False\nbert.encoder.layer.5.attention.self.value.bias False\nbert.encoder.layer.5.attention.output.dense.weight False\nbert.encoder.layer.5.attention.output.dense.bias False\nbert.encoder.layer.5.attention.output.LayerNorm.weight False\nbert.encoder.layer.5.attention.output.LayerNorm.bias False\nbert.encoder.layer.5.intermediate.dense.weight False\nbert.encoder.layer.5.intermediate.dense.bias False\nbert.encoder.layer.5.output.dense.weight False\nbert.encoder.layer.5.output.dense.bias False\nbert.encoder.layer.5.output.LayerNorm.weight False\nbert.encoder.layer.5.output.LayerNorm.bias False\nbert.encoder.layer.6.attention.self.query.weight False\nbert.encoder.layer.6.attention.self.query.bias False\nbert.encoder.layer.6.attention.self.key.weight False\nbert.encoder.layer.6.attention.self.key.bias False\nbert.encoder.layer.6.attention.self.value.weight False\nbert.encoder.layer.6.attention.self.value.bias False\nbert.encoder.layer.6.attention.output.dense.weight False\nbert.encoder.layer.6.attention.output.dense.bias False\nbert.encoder.layer.6.attention.output.LayerNorm.weight False\nbert.encoder.layer.6.attention.output.LayerNorm.bias False\nbert.encoder.layer.6.intermediate.dense.weight False\nbert.encoder.layer.6.intermediate.dense.bias False\nbert.encoder.layer.6.output.dense.weight False\nbert.encoder.layer.6.output.dense.bias False\nbert.encoder.layer.6.output.LayerNorm.weight False\nbert.encoder.layer.6.output.LayerNorm.bias False\nbert.encoder.layer.7.attention.self.query.weight False\nbert.encoder.layer.7.attention.self.query.bias False\nbert.encoder.layer.7.attention.self.key.weight False\nbert.encoder.layer.7.attention.self.key.bias False\nbert.encoder.layer.7.attention.self.value.weight False\nbert.encoder.layer.7.attention.self.value.bias False\nbert.encoder.layer.7.attention.output.dense.weight False\nbert.encoder.layer.7.attention.output.dense.bias False\nbert.encoder.layer.7.attention.output.LayerNorm.weight False\nbert.encoder.layer.7.attention.output.LayerNorm.bias False\nbert.encoder.layer.7.intermediate.dense.weight False\nbert.encoder.layer.7.intermediate.dense.bias False\nbert.encoder.layer.7.output.dense.weight False\nbert.encoder.layer.7.output.dense.bias False\nbert.encoder.layer.7.output.LayerNorm.weight False\nbert.encoder.layer.7.output.LayerNorm.bias False\nbert.encoder.layer.8.attention.self.query.weight True\nbert.encoder.layer.8.attention.self.query.bias True\nbert.encoder.layer.8.attention.self.key.weight True\nbert.encoder.layer.8.attention.self.key.bias True\nbert.encoder.layer.8.attention.self.value.weight True\nbert.encoder.layer.8.attention.self.value.bias True\nbert.encoder.layer.8.attention.output.dense.weight True\nbert.encoder.layer.8.attention.output.dense.bias True\nbert.encoder.layer.8.attention.output.LayerNorm.weight True\nbert.encoder.layer.8.attention.output.LayerNorm.bias True\nbert.encoder.layer.8.intermediate.dense.weight True\nbert.encoder.layer.8.intermediate.dense.bias True\nbert.encoder.layer.8.output.dense.weight True\nbert.encoder.layer.8.output.dense.bias True\nbert.encoder.layer.8.output.LayerNorm.weight True\nbert.encoder.layer.8.output.LayerNorm.bias True\nbert.encoder.layer.9.attention.self.query.weight True\nbert.encoder.layer.9.attention.self.query.bias True\nbert.encoder.layer.9.attention.self.key.weight True\nbert.encoder.layer.9.attention.self.key.bias True\nbert.encoder.layer.9.attention.self.value.weight True\nbert.encoder.layer.9.attention.self.value.bias True\nbert.encoder.layer.9.attention.output.dense.weight True\nbert.encoder.layer.9.attention.output.dense.bias True\nbert.encoder.layer.9.attention.output.LayerNorm.weight True\nbert.encoder.layer.9.attention.output.LayerNorm.bias True\nbert.encoder.layer.9.intermediate.dense.weight True\nbert.encoder.layer.9.intermediate.dense.bias True\nbert.encoder.layer.9.output.dense.weight True\nbert.encoder.layer.9.output.dense.bias True\nbert.encoder.layer.9.output.LayerNorm.weight True\nbert.encoder.layer.9.output.LayerNorm.bias True\nbert.encoder.layer.10.attention.self.query.weight True\nbert.encoder.layer.10.attention.self.query.bias True\nbert.encoder.layer.10.attention.self.key.weight True\nbert.encoder.layer.10.attention.self.key.bias True\nbert.encoder.layer.10.attention.self.value.weight True\nbert.encoder.layer.10.attention.self.value.bias True\nbert.encoder.layer.10.attention.output.dense.weight True\nbert.encoder.layer.10.attention.output.dense.bias True\nbert.encoder.layer.10.attention.output.LayerNorm.weight True\nbert.encoder.layer.10.attention.output.LayerNorm.bias True\nbert.encoder.layer.10.intermediate.dense.weight True\nbert.encoder.layer.10.intermediate.dense.bias True\nbert.encoder.layer.10.output.dense.weight True\nbert.encoder.layer.10.output.dense.bias True\nbert.encoder.layer.10.output.LayerNorm.weight True\nbert.encoder.layer.10.output.LayerNorm.bias True\nbert.encoder.layer.11.attention.self.query.weight True\nbert.encoder.layer.11.attention.self.query.bias True\nbert.encoder.layer.11.attention.self.key.weight True\nbert.encoder.layer.11.attention.self.key.bias True\nbert.encoder.layer.11.attention.self.value.weight True\nbert.encoder.layer.11.attention.self.value.bias True\nbert.encoder.layer.11.attention.output.dense.weight True\nbert.encoder.layer.11.attention.output.dense.bias True\nbert.encoder.layer.11.attention.output.LayerNorm.weight True\nbert.encoder.layer.11.attention.output.LayerNorm.bias True\nbert.encoder.layer.11.intermediate.dense.weight True\nbert.encoder.layer.11.intermediate.dense.bias True\nbert.encoder.layer.11.output.dense.weight True\nbert.encoder.layer.11.output.dense.bias True\nbert.encoder.layer.11.output.LayerNorm.weight True\nbert.encoder.layer.11.output.LayerNorm.bias True\nattention.attn.weight True\nattention.attn_combine.weight True\nfc.0.fc.weight True\nfc.0.fc.bias True\nfc.1.fc.weight True\nfc.1.fc.bias True\nfc.2.weight True\nfc.2.bias True\n"
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_data_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1 / 8\n--------------------\nBatch Train loss: 0.6960     Class 1 prop: 0.4375     ACC: 0.5625     F1: 0.6316     AUC: 0.4104     \nBatch Train loss: 0.6903     Class 1 prop: 0.5312     ACC: 0.8125     F1: 0.8125     AUC: 0.8509     \nBatch Train loss: 0.6896     Class 1 prop: 0.4688     ACC: 0.8438     F1: 0.8387     AUC: 0.8152     \nBatch Train loss: 0.6837     Class 1 prop: 0.6250     ACC: 0.9375     F1: 0.9474     AUC: 0.9866     \nBatch Train loss: 0.6776     Class 1 prop: 0.5625     ACC: 0.9688     F1: 0.9714     AUC: 0.9897     \n"
    }
   ],
   "source": [
    "%%time\n",
    "history = training.train_model(epochs, model, train_data_loader, val_data_loader, loss_fn, optimizer, device, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c8f9b130421e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training_history.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(history, open('training_history.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-856c60b3029a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training_history.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "history = pickle.load(history, open('training_history.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-701806d04afc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'val loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(list(range(epochs)), history['train_loss'], label = 'train loss')\n",
    "plt.plot(list(range(epochs)), history['val_loss'], label = 'val loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-bb13a9502384>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_f1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train f1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_f1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'val f1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(list(range(epochs)), history['train_f1'], label = 'train f1')\n",
    "plt.plot(list(range(epochs)), history['val_f1'], label = 'val f1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-8fd44e8f8df7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_auc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train auc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_auc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'val auc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(list(range(epochs)), history['train_auc'], label = 'train auc')\n",
    "plt.plot(list(range(epochs)), history['val_auc'], label = 'val auc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = models.HIBERT(**params_dict)\n",
    "model_trained.load_state_dict(torch.load('best_model_state.bin'))\n",
    "\n",
    "best_threshold = pickle.load(history, open('best_threshold.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-e72d4585f431>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-32-e72d4585f431>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    predictions = training.\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "predictions = training.pred_model(\n",
    "    model = model_trained, \n",
    "    data_loader = test_data_loader, \n",
    "    device = device, \n",
    "    best_threshold = best_threshold)"
   ]
  }
 ]
}